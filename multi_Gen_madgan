dir_name = "Model7" #location to save the model in Google Drive


n_Gen = 4 #number of generators
h_Dim = 128 #dimention of hidden layers
latent_dim =  64 # dimention of input noise
size_dataset =  200000 #size of dataset
batch_size = 128 #number of batches

steps_per_epoch = (size_dataset//batch_size)//n_Gen


import matplotlib.pyplot as plt
from sklearn import mixture

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Model
from google.colab import output

import os


# for saving GIF
!pip install pygifsicle
!sudo apt-get install gifsicle
import imageio
import glob
from pygifsicle import optimize
output.clear()
print("Done")



print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print(tf.test.gpu_device_name())



from google.colab import drive
drive.mount('/content/drive')

if os.path.exists(f'/content/drive/MyDrive/{dir_name}') == False:
    os.mkdir(f'/content/drive/MyDrive/{dir_name}')
    os.mkdir(f'/content/drive/MyDrive/{dir_name}/Charts')
    
    
    
def dataset1_func(size_dataset, random_state = None):
    #comment the random_state below for final execution
    gmm = mixture.GaussianMixture(5, random_state = random_state)
    gmm.means_ = tf.constant([[10], [20], [60], [80], [110]])
    gmm.covariances_ = tf.constant([[[3]], [[3]], [[2]], [[2]], [[1]]]) ** 2
    gmm.weights_ = tf.constant([0.2, 0.2, 0.2, 0.2, 0.2])
    gmm.get_params()
    X = gmm.sample(size_dataset)
    # plt.hist(X[0], size_dataset, histtype='stepfilled', alpha=1)
    return X[0]



# generate points in latent space as input for the generator
def generate_latent_points(latent_dim, batch_size, n_Gen):
    # generate points in the latent space
    x_input = tf.random.uniform(minval = -1, maxval = 1, dtype=tf.float64, shape = [n_Gen, batch_size, latent_dim])
    # reshape into a batch of inputs for the network
    noise = []
    for i in range(n_Gen):
        noise.append(x_input[i])
    return noise
    
    
    

class GANMonitor1(tf.keras.callbacks.Callback):
    def __init__(self, dataset, plot_freq = 1, num_samples = 20000, latent_dim = 64, n_Gen = 4, dir_name = 'Model'):
        self.dataset = dataset
        self.num_samples = num_samples
        self.latent_dim = latent_dim
        self.n_Gen = n_Gen
        self.dir_name = dir_name
        self.plot_freq = plot_freq

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.plot_freq == 0:
            random_latent_vectors = generate_latent_points(self.latent_dim, self.num_samples, self.n_Gen)
            plt.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            plt.xlim(0,120)
            plt.ylim(0,0.2)
            for g in range(self.n_Gen):
                generated_samples = self.model.generators[g](random_latent_vectors[g])
                plt.hist(generated_samples.numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            plt.savefig(f'/content/drive/MyDrive/{self.dir_name}/Charts/chart_{(epoch + 1):04}.png', dpi=200, format="png")
            
            # To show the plots in colab comment line below and uncomment the next line
            # plt.close()
            plt.show()
            
            
            
class GANMonitor2(tf.keras.callbacks.Callback):
    def __init__(self, dataset, plot_freq = 1, num_samples = 20000, latent_dim = 64, n_Gen = 4, dir_name = 'Model'):
        self.dataset = dataset
        self.num_samples = num_samples
        self.latent_dim = latent_dim
        self.n_Gen = n_Gen
        self.dir_name = dir_name
        self.plot_freq = plot_freq

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.plot_freq == 0:
            random_latent_vectors = generate_latent_points(self.latent_dim, self.num_samples, self.n_Gen)

            generated_samples = []
            for g in range(self.n_Gen):
                generated_samples.append(self.model.generators[g](random_latent_vectors[g]))

            combined_generated_samples = tf.concat([generated_samples[g] for g in range(self.n_Gen)], axis=0)

            ax1 = plt.subplot(313)
            ax1.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            ax1.hist(combined_generated_samples.numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            ax1.set_title('All Generators together')
            ax1.set_xlim([0,120])
            ax1.set_ylim([0,0.2])

            ax2 = plt.subplot(321)
            ax2.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            ax2.hist(generated_samples[0].numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            ax2.set_title('First Generator')
            ax2.set_xlim([0,120])
            ax2.set_ylim([0,0.2])

            ax3 = plt.subplot(322)
            ax3.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            ax3.hist(generated_samples[1].numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            ax3.set_title('Second Generator')
            ax3.set_xlim([0,120])
            ax3.set_ylim([0,0.2])

            ax4 = plt.subplot(323)
            ax4.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            ax4.hist(generated_samples[2].numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            ax4.set_title('Third Generator')
            ax4.set_xlim([0,120])
            ax4.set_ylim([0,0.2])

            ax5 = plt.subplot(324)
            ax5.hist(self.dataset, 1200, histtype='bar', density=True, alpha=0.5)
            ax5.hist(generated_samples[3].numpy(), 1200, histtype='bar', density=True, alpha=0.5)
            ax5.set_title('Forth Generator')
            ax5.set_xlim([0,120])
            ax5.set_ylim([0,0.2])

            plt.tight_layout()            
            plt.savefig(f'/content/drive/MyDrive/{self.dir_name}/Charts/chart_{(epoch + 1):04}.png', dpi=200, format="png")

            # To show the plots in colab comment line below and uncomment the next line
            # plt.close()
            plt.show()
            
            

@tf.function
def Generators_loss_function(y_true, y_pred):
    logarithm = -tf.math.log(y_pred[:,-1] + 1e-15)
    return tf.reduce_mean(logarithm, axis=-1)
    
    

# define the standalone discriminator model
def define_discriminator(n_Gen, h_Dim):
    inp = Input(shape=(1,))
    x = Dense(h_Dim,input_shape=(1,))(inp)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(h_Dim)(x)
    x = LeakyReLU(alpha=0.2)(x)
    out = Dense(n_Gen+1, activation="softmax")(x)
    model = Model(inp, out, name="discriminator")
    return model
    
    
    
def define_generators(n_Gen, latent_dim, h_Dim):
    mid_layer1 = Dense(h_Dim, name = "hidden_1", activation = 'elu', input_shape=(latent_dim,))
    mid_layer2 = Dense(h_Dim, name = "hidden_2", activation = 'elu')

    models = []
    for g in range(n_Gen):
        input = Input(shape=(latent_dim,), dtype = tf.float64, name=f"input_{g}")
        x = mid_layer1(input)
        x = mid_layer2(x)
        x = Dense(1, dtype = tf.float64, name=f"generator_output_{g}")(x)
        models.append(Model(input, x, name = f"generator{g}"))
    return models
    
    
    
class MADGAN(tf.keras.Model):
    def __init__(self, discriminator, generators, latent_dim, n_Gen):
        super(MADGAN, self).__init__()
        self.discriminator = discriminator
        self.generators = generators
        self.latent_dim = latent_dim
        self.n_Gen = n_Gen

    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):
        super(MADGAN, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.d_loss_fn = d_loss_fn
        self.g_loss_fn = g_loss_fn

    def train_step(self, data):
        # Unpack the data
        if isinstance(data, tuple):
            X = data[0]
        else:
            X = data
        
        # Get the batch size
        batch_size = tf.shape(X)[0]
        # Sample random points in the latent space
        random_latent_vectors = generate_latent_points(self.latent_dim, batch_size//self.n_Gen, self.n_Gen)
        # Decode them to fake generator output
        x_generator = []
        for g in range(self.n_Gen):
            x_generator.append(self.generators[g](random_latent_vectors[g]))

        # Combine them with real samples
        combined_samples = tf.concat([x_generator[g] for g in range(self.n_Gen)] + 
                                     [X], 
                                     axis=0
                                     )

        # Assemble labels discriminating real from fake samples
        labels = tf.concat([tf.one_hot(g * tf.ones(batch_size//self.n_Gen, dtype=tf.int32), self.n_Gen + 1) for g in range(self.n_Gen)] + 
                    [tf.one_hot(self.n_Gen * tf.ones(batch_size, dtype=tf.int32), self.n_Gen + 1)], 
                    axis=0
                    )

        # Add random noise to the labels - important trick!
        labels += 0.05 * tf.random.uniform(shape = tf.shape(labels), minval = -1, maxval = 1)

        #######################
        # Train Discriminator #
        #######################
        
        # make weights in the discriminator trainable
        with tf.GradientTape() as tape:
            # Discriminator forward pass
            predictions = self.discriminator(combined_samples)

            # Compute the loss value
            # (the loss function is configured in `compile()`)
            d_loss = self.d_loss_fn(labels, predictions)

        # Compute gradients
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)


        # Update weights
        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))

        #######################
        #   Train Generator   #
        #######################

        # Assemble labels that say "all real samples"
        misleading_labels =  tf.one_hot(self.n_Gen * tf.ones(batch_size//self.n_Gen, dtype=tf.int32), self.n_Gen + 1)

        # (note that we should *not* update the weights of the discriminator)!
        g_loss_list = []
        for g in range(self.n_Gen):
            with tf.GradientTape() as tape:
                # Generator[g] and discriminator forward pass
                predictions = self.discriminator(self.generators[g](random_latent_vectors[g]))

                # Compute the loss value
                # (the loss function is configured in `compile()`)
                g_loss = self.g_loss_fn(misleading_labels, predictions)

            # Compute gradients
            grads = tape.gradient(g_loss, self.generators[g].trainable_weights)
            # Update weights
            self.g_optimizer[g].apply_gradients(zip(grads, self.generators[g].trainable_weights))
            g_loss_list.append(g_loss)

        mydict = {f"g_loss{g}": g_loss_list[g] for g in range(self.n_Gen)}
        mydict.update({"d_loss": d_loss})
        return mydict
        
        
        
        
# Loading data
data = dataset1_func(size_dataset)
# Changing numpy dataset to tf.DATASET type and Shuffling dataset for training
dataset = tf.data.Dataset.from_tensor_slices(data) 
dataset = dataset.repeat().shuffle(10 * size_dataset, reshuffle_each_iteration=True).batch(n_Gen * batch_size, drop_remainder=True)

# Creating Discriminator and Generator
discriminator = define_discriminator(n_Gen, h_Dim)
generators = define_generators(n_Gen, latent_dim, h_Dim)

# creating MADGAN
madgan = MADGAN(discriminator = discriminator, generators = generators, 
                latent_dim = latent_dim, n_Gen = n_Gen)

madgan.compile(
    d_optimizer = Adam(learning_rate=1e-4, beta_1=0.5),
    g_optimizer = [Adam(learning_rate=1e-4, beta_1=0.5) for g in range(n_Gen)],
    d_loss_fn = CategoricalCrossentropy(),
    g_loss_fn = Generators_loss_function
)

# saved model directory
checkpoint_filepath = f'/content/drive/MyDrive/{dir_name}/checkpoint'

# callbacks are functions that run at end of each epoch
my_callbacks = [
    # This callback is for ploting generators' output every epoch
    GANMonitor2(dataset = data, plot_freq = 5, num_samples = 10000, latent_dim = latent_dim, n_Gen = n_Gen, dir_name = dir_name),
    # This callback is for Saving the model every 15 epochs
    tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath, save_freq = 20, save_weights_only=True),
]

# # Loading previous saved model for resume training
# if os.path.exists(checkpoint_filepath):
#     madgan.load_weights(checkpoint_filepath)

# train the model
madgan.fit(dataset, epochs = 200, initial_epoch = 0, steps_per_epoch = steps_per_epoch, verbose = 1, callbacks = my_callbacks)










save gif

anim_file = 'madgan.gif'

with imageio.get_writer(f'/content/drive/MyDrive/{dir_name}/{anim_file}', mode='I') as writer:
    filenames = glob.glob(f'/content/drive/MyDrive/{dir_name}/Charts/chart*.png')
    filenames = sorted(filenames)
    for filename in filenames:
        image = imageio.imread(filename)
        # image = image[::4,::4,:]
        writer.append_data(image)
        writer.append_data(image)
    for i in range(20):
        writer.append_data(image)   

# Reduce GIF size
optimize(f'/content/drive/MyDrive/{dir_name}/{anim_file}')
    
   
